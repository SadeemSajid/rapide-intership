{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3943d4-f8ab-4d1c-b486-163e397e4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Document loading, retrieval methods and text splitting\n",
    "# %pip install -qU langchain langchain_community\n",
    "\n",
    "# # Local vector store via Chroma\n",
    "# %pip install -qU langchain_chroma\n",
    "\n",
    "# # Local inference and embeddings via Ollama\n",
    "# %pip install -qU langchain_ollama\n",
    "\n",
    "# %pip install python-docx\n",
    "\n",
    "# %pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778cc1ec-877f-4be7-ac37-50de019fddce",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40a3dca2-ba68-43d5-bed0-8648b1c09a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import streamlit as st\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07a5c8-bca0-413d-a93e-2a9a382c94f0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba4a78e4-d9fc-487e-8e70-57283a53e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a helper function to concatenate docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def clean_response(response):\n",
    "    # Remove everything between <think> and </think> tags (including the tags)\n",
    "    cleaned_response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove any leading/trailing whitespace\n",
    "    cleaned_response = cleaned_response.strip()\n",
    "    \n",
    "    return cleaned_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd286ce-424b-4b5d-9daf-d100feedb872",
   "metadata": {},
   "source": [
    "# Developing RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f349c63-7f4c-45f2-9846-d996557d8fca",
   "metadata": {},
   "source": [
    "## Loading LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72089af2-4a96-435b-ab10-0e88be9fd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama with my local model\n",
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348d313-696f-4436-a467-7b9b083737f0",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86666ff-04c4-4240-8f10-5b0a32627eb4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load the .docx file\n",
    "def load_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])\n",
    "    return text\n",
    "\n",
    "# Load the company manual\n",
    "text = load_docx('manual.docx')\n",
    "\n",
    "# Split the text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd893e-5279-4cb0-8a48-4b0e74530605",
   "metadata": {},
   "source": [
    "## Making vector embeddings & intialising vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fece65-108a-4984-9161-9a102d457b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make embeddings\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Make vector store\n",
    "vectorstore = Chroma.from_texts(all_splits, local_embeddings)\n",
    "\n",
    "# Make a retriever from the store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d89ab-24d7-418d-9b53-e33f767953d1",
   "metadata": {},
   "source": [
    "## Developing a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee3c06ac-b0d6-409d-b59e-437dff3d2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developing a chain\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | clean_response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4883f8e-418f-4b11-b719-e75255c4f276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document does not mention anything about having pet animals.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "question = \"What does the document mention about having pet animals?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
